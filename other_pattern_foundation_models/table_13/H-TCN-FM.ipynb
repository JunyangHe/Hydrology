{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81a4550f-d0bd-4e25-ac12-8003081f4016",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "from neuralforecast.losses.pytorch import BasePointLoss, _weighted_mean\n",
    "import torch\n",
    "from typing import Union\n",
    "import warnings\n",
    "# Filter specific warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"val_check_steps is greater than max_steps\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"The 'val_dataloader' does not have many workers\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"The 'train_dataloader' does not have many workers\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"The number of training batches\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"`Trainer.fit` stopped: `max_steps=16` reached.\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"Trying to infer the `batch_size` from an ambiguous collection\")\n",
    "import logging\n",
    "logging.getLogger(\"pytorch_lightning\").setLevel(logging.CRITICAL)\n",
    "import pytorch_lightning as pl\n",
    "# Trainer configuration\n",
    "trainer = pl.Trainer(logger=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b997f44d-0dd2-4b61-aa70-bf5703baaa53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded from file.\n"
     ]
    }
   ],
   "source": [
    "# Define the file name\n",
    "file_path = 'BasicInputTimeSeries.npy'\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(file_path):\n",
    "    # Load the .npy file with allow_pickle=True\n",
    "    time_series_data = np.load(file_path, allow_pickle=True)\n",
    "    print(\"Data loaded from file.\")\n",
    "else:\n",
    "    print(\"File not found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "276ec650-0257-4b2f-8957-44bb0f77a778",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed : 100\n",
      "EMA_2\n",
      "EMA_4\n",
      "EMA_6\n",
      "EMA_8\n",
      "EMA_10\n",
      "EMA_20\n",
      "EMA_50\n",
      "DataFrame:\n",
      "        srad(W/m2) tmax(C) tmin(C)  vp(Pa)  QObs(mm/d)         ds unique_id  \\\n",
      "0           237.53   12.75   -1.37  599.13    0.376714 1989-11-02   1013500   \n",
      "1            99.59     8.1   -0.31  561.45    0.379961 1989-11-03   1013500   \n",
      "2           129.07    3.35   -4.01  425.97    0.397282 1989-11-04   1013500   \n",
      "3           193.49   -0.75   -9.83   303.0    0.405942 1989-11-05   1013500   \n",
      "4           159.91    3.82   -5.66  425.19    0.422179 1989-11-06   1013500   \n",
      "...            ...     ...     ...     ...         ...        ...       ...   \n",
      "4696995     125.72    4.86    0.39  604.28   22.806309 2008-12-27  14400000   \n",
      "4696996     101.25    6.06    2.83  702.39  115.775145 2008-12-28  14400000   \n",
      "4696997     140.54    7.87    2.46  703.56  134.606043 2008-12-29  14400000   \n",
      "4696998     171.69    6.96   -3.14  532.92    54.40037 2008-12-30  14400000   \n",
      "4696999     174.73    8.15    0.13  599.32   29.920203 2008-12-31  14400000   \n",
      "\n",
      "              y       EMA_2       EMA_4      EMA_6      EMA_8     EMA_10  \\\n",
      "0          0.81    2.134198    2.499430   2.313171   2.148989   2.051240   \n",
      "1          6.44    5.004733    4.075658   3.492265   3.102547   2.849196   \n",
      "2          2.38    3.254911    3.397395   3.174475   2.941981   2.763888   \n",
      "3          0.18    1.204970    2.110437   2.318911   2.328208   2.294090   \n",
      "4          1.88    1.654990    2.018262   2.193508   2.228606   2.218801   \n",
      "...         ...         ...         ...        ...        ...        ...   \n",
      "4696995   87.46   68.438059   52.990167  45.472344  40.572067  36.916003   \n",
      "4696996  141.28  116.999353   88.306100  72.845960  62.951608  55.891275   \n",
      "4696997  153.34  141.226451  114.319660  95.844257  83.037917  73.609225   \n",
      "4696998   52.62   82.155484   89.639796  83.494469  76.278380  69.793002   \n",
      "4696999   12.77   35.898495   58.891878  63.287478  62.165407  59.425184   \n",
      "\n",
      "            EMA_20     EMA_50  \n",
      "0         2.213850   4.225196  \n",
      "1         2.616341   4.312051  \n",
      "2         2.593832   4.236284  \n",
      "3         2.363943   4.077214  \n",
      "4         2.317854   3.991049  \n",
      "...            ...        ...  \n",
      "4696995  26.258824  16.018381  \n",
      "4696996  37.213221  20.930601  \n",
      "4696997  48.272915  26.123127  \n",
      "4696998  48.686923  27.162220  \n",
      "4696999  45.266263  26.597819  \n",
      "\n",
      "[4697000 rows x 15 columns]\n"
     ]
    }
   ],
   "source": [
    "#-------------------------------------------------------------\n",
    "seed = 100\n",
    "# Set the random seed for Python's random module\n",
    "random.seed(seed)\n",
    "# Set the random seed for NumPy\n",
    "np.random.seed(seed)\n",
    "print('seed :',seed)\n",
    "#-------------------------------------------------------------\n",
    "\n",
    "    \n",
    "# Define the column names\n",
    "columns = [\"Year_Mnth_Day\", \"basin_id\", \"prcp(mm/day)\", \"srad(W/m2)\", \"tmax(C)\", \"tmin(C)\", \"vp(Pa)\", \"QObs(mm/d)\"]\n",
    "# Create the DataFrame\n",
    "df = pd.DataFrame(time_series_data, columns=columns)\n",
    "\n",
    "# Deleting the numpy array from memory\n",
    "del time_series_data\n",
    "\n",
    "df['ds'] = pd.to_datetime(df['Year_Mnth_Day'])\n",
    "df['unique_id'] = df['basin_id']\n",
    "df['y'] = df[\"prcp(mm/day)\"].astype(float)\n",
    "df.drop(['Year_Mnth_Day', 'basin_id', \"prcp(mm/day)\", ], axis=1, inplace=True) # \"srad(W/m2)\", \"tmax(C)\", \"tmin(C)\", \"vp(Pa)\", \"QObs(mm/d)\"\n",
    "\n",
    "# for col in df.columns:\n",
    "#     print(col)\n",
    "#     if col != 'ds' and col !='unique_id':\n",
    "#         # Global Normalization\n",
    "#         scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "#         df[col] = scaler.fit_transform(df[[col]])\n",
    "\n",
    "\n",
    "# Sort the DataFrame by 'unique_id' and 'ds' to ensure correct order\n",
    "df.sort_values(by=['unique_id', 'ds'], inplace=True)\n",
    "\n",
    "# Function to split each group\n",
    "def split_train_test(data, n):\n",
    "    train_frames = []\n",
    "    test_frames = []\n",
    "    for _, group in data.groupby('unique_id'):\n",
    "        train, test = group[:-n], group[-n:]\n",
    "        train_frames.append(train)\n",
    "        test_frames.append(test)\n",
    "    train_df = pd.concat(train_frames)\n",
    "    test_df = pd.concat(test_frames)\n",
    "    train_df.reset_index(inplace=True, drop=True)\n",
    "    test_df.reset_index(inplace=True, drop=True)\n",
    "    return train_df, test_df\n",
    "\n",
    "# Function to drop the last n values fron each group\n",
    "def drop_last_n(data, n):\n",
    "    train_frames = []\n",
    "    for _, group in data.groupby('unique_id'):\n",
    "        train = group[:-n]\n",
    "        train_frames.append(train)\n",
    "    train_df = pd.concat(train_frames)\n",
    "    train_df.reset_index(inplace=True, drop=True)\n",
    "    return train_df\n",
    "\n",
    "\n",
    "    \n",
    "# Set 'ds' as the index if you plan to use time-based indexing\n",
    "df.set_index('ds', inplace=True, drop=False)\n",
    "\n",
    "# Function to calculate EMA for each group\n",
    "def calculate_ema(group, span):\n",
    "    return group.ewm(span=span, adjust=False).mean()\n",
    "\n",
    "# Apply the function to each group for multiple spans\n",
    "span_settings = [2, 4, 6, 8, 10, 20, 50]  # Example spans\n",
    "for span in span_settings:\n",
    "    print(f'EMA_{span}')\n",
    "    df[f'EMA_{span}'] = df.groupby('unique_id')['y'].transform(lambda x: calculate_ema(x, span))\n",
    "    \n",
    "df.reset_index(inplace=True, drop=True)\n",
    "   \n",
    "    \n",
    "# Number of records to be taken as test data for each unique_id\n",
    "n = 3000 \n",
    "n_time_series = 671\n",
    "len_time_series = 7000\n",
    "_, df = split_train_test(df, len_time_series)\n",
    "\n",
    "train_df, test_df = split_train_test(df, n)\n",
    "\n",
    "\n",
    "print(\"DataFrame:\")\n",
    "print(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09ca81f2-11ae-4227-9765-15ce322eac8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank: 0] Seed set to 10\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5734dbb56fd14e07bab16bba862b9ccc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=2100` reached.\n"
     ]
    }
   ],
   "source": [
    "from neuralforecast.core import NeuralForecast\n",
    "from neuralforecast.models import VanillaTransformer, TCN\n",
    "from neuralforecast.losses.pytorch import MAE, MSE, QuantileLoss\n",
    "from neuralforecast.auto import AutoNHITS, AutoPatchTST\n",
    "from ray import tune\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "horizon = 1\n",
    "models = [TCN(h = horizon,  context_size=1024, input_size=1024 , max_steps=21*100, learning_rate=1e-4, loss= MSE(), random_seed=10, batch_size=32, scaler_type=None, \n",
    "                    hist_exog_list =[\"EMA_2\",\"EMA_4\",\"EMA_6\",\"EMA_8\",\"EMA_10\",\"EMA_20\",\"EMA_50\", \"srad(W/m2)\", \"tmax(C)\", \"tmin(C)\", \"vp(Pa)\", \"QObs(mm/d)\"])]\n",
    "nf = NeuralForecast(models=models, freq='1D', local_scaler_type=None)\n",
    "\n",
    "nf.fit(train_df, val_size=0)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ab804a-5bfd-4935-b1a1-51f00a468789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty DataFrame to store all predictions\n",
    "all_preds = pd.DataFrame()\n",
    "for i in range(n, 0, -horizon):\n",
    "    print(\"re-fit and test :\", i)\n",
    "    _, re_train_df = split_train_test(df, i+1024)\n",
    "    data_part = drop_last_n(re_train_df, i)\n",
    "    preds = nf.predict(data_part)\n",
    "    preds = preds.reset_index(drop=False)\n",
    "    all_preds = pd.concat([all_preds, preds], ignore_index=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6fd75606-3e4a-401b-921b-bd00289a9450",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "all_preds = all_preds.sort_values(by=['unique_id', 'ds'], ascending=[True, True])\n",
    "results_df = test_df.copy()\n",
    "results_df['TCN'] = all_preds['TCN'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3cf23a8b-0f5c-42c8-a36d-80558e94ff03",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_actual_y = results_df['y']\n",
    "test_predicted_y = results_df['TCN']\n",
    "train_actual_y = train_df['y']\n",
    "\n",
    "\n",
    "index_date_test = results_df[results_df['unique_id'] == 1013500]['ds']\n",
    "index_date_train = train_df['ds'][:len_time_series-n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87455685-fb7f-4ff1-9e25-df7d558f0e75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  30.177716735702713\n",
      "MAE:  2.6137388779363464\n",
      "NNSE:  0.6068980608016832\n",
      "R²:  0.3522768244158696\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def mean_absolute_error(y_true, y_pred):\n",
    "    return np.mean(np.abs(y_true - y_pred))\n",
    "\n",
    "def normalized_nash_sutcliffe_efficiencySTavg(y_true, y_pred):  # axis 0 space 1 time\n",
    "    NSE = 1 - np.sum(np.square(y_true - y_pred)) / np.sum(np.square(y_true - np.mean(y_true)))\n",
    "    return 1 / (2 - NSE)\n",
    "\n",
    "def mean_squared_error(y_true, y_pred):\n",
    "    return np.mean(np.square(y_true - y_pred))\n",
    "\n",
    "def r_squared(y_true, y_pred):\n",
    "    ss_res = np.sum(np.square(y_true - y_pred))\n",
    "    ss_tot = np.sum(np.square(y_true - np.mean(y_true)))\n",
    "    return 1 - (ss_res / ss_tot)\n",
    "\n",
    "# Assuming your data is organized in results_df and reshaped\n",
    "Y_ACTUAL = results_df['y'].values.reshape(n_time_series, n)\n",
    "Y_HAT = results_df['TCN'].values.reshape(n_time_series, n)\n",
    "\n",
    "MAE = mean_absolute_error(Y_ACTUAL, Y_HAT)\n",
    "MSE = mean_squared_error(Y_ACTUAL, Y_HAT)\n",
    "NNSE = normalized_nash_sutcliffe_efficiencySTavg(Y_ACTUAL, Y_HAT)\n",
    "R2 = r_squared(Y_ACTUAL, Y_HAT)\n",
    "\n",
    "print(\"MSE: \", MSE)\n",
    "print(\"MAE: \", MAE)\n",
    "print(\"NNSE: \", NNSE)\n",
    "print(\"R²: \", R2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c0d506d-0f7f-42d7-9c8f-9d65f12cf432",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_pickle('TCN_prcp.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbb84fe-0d79-45d4-990e-19161a621b3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch 2.0.1",
   "language": "python",
   "name": "pytorch-2.0.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
