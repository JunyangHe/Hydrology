{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "buHF4ItfE6Ux"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import json\n",
        "import pandas as pd\n",
        "import os\n",
        "from pathlib import Path, PosixPath\n",
        "from tqdm import tqdm\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "from typing import List\n",
        "import gc\n",
        "import time\n",
        "import datetime\n",
        "from datetime import timedelta,date,datetime\n",
        "!pip install cloudmesh-common -U\n",
        "from cloudmesh.common.StopWatch import StopWatch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iFAYc2YqSSfH"
      },
      "outputs": [],
      "source": [
        "CREATE_CSV_FILES = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3obgW9mE6U1"
      },
      "source": [
        "# create csv files\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51sKc_YjT6Om"
      },
      "source": [
        "## read forcing files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iEigB6p_E6U5"
      },
      "outputs": [],
      "source": [
        "def read_forcing_file(fpath: PosixPath):\n",
        "    basin_id = int(fpath.name.split('_')[0])\n",
        "    data = pd.read_csv(fpath, delim_whitespace=True,skiprows=3, parse_dates=[[0,1,2]])\n",
        "\n",
        "    gauge_lat, gauge_elv_m, basin_area_m2  = np.genfromtxt(fpath, max_rows=3)\n",
        "\n",
        "    data[\"basin_id\"] = basin_id\n",
        "    data[\"gauge_lat\"] = gauge_lat\n",
        "    data[\"gauge_elv(m)\"] = gauge_elv_m\n",
        "    data[\"basin_area(m2)\"] = int(basin_area_m2)\n",
        "\n",
        "    data.set_index(['basin_id', 'Year_Mnth_Day'], inplace=True)\n",
        "\n",
        "    return data\n",
        "\n",
        "def read_forcing_data(data_dir: str):\n",
        "    files = list(Path(data_dir).glob('**/*_forcing_leap.txt'))\n",
        "\n",
        "    data_list = []\n",
        "\n",
        "    pbar = tqdm(files, file=sys.stdout, position=0)\n",
        "    for fpath in pbar:\n",
        "        pbar.set_description(\"process \" + fpath.name)\n",
        "        data = read_forcing_file(fpath)\n",
        "\n",
        "        data_list.append(data)\n",
        "\n",
        "    forcing_data = pd.concat(data_list, axis=0, copy=False)\n",
        "\n",
        "    return forcing_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7-POP3KvE6U8"
      },
      "outputs": [],
      "source": [
        "if CREATE_CSV_FILES:\n",
        "  maurer_ext_path = \"/N/u2/d/dnperera/Colab Datasets/maurer_extended\"\n",
        "  forcing_data = read_forcing_data(maurer_ext_path)\n",
        "  forcing_data.to_csv(maurer_ext_path + \"/forcing_data.csv\")\n",
        "  forcing_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FT2YhiUDE6VG"
      },
      "source": [
        "## read discharge files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "46qmroozE6VG"
      },
      "outputs": [],
      "source": [
        "def read_discharge_file(fpath: PosixPath, area):\n",
        "    data = pd.read_csv(fpath, delim_whitespace=True, parse_dates=[[1,2,3]], header=0,\n",
        "                       names=[\"basin_id\", \"Year\", \"Mnth\", \"Day\", \"QObs(mm/d)\", \"flag\"])\n",
        "    # normalize discharge from cubic feed per second to mm per day\n",
        "    data[\"QObs(mm/d)\"] = 28316846.592 * data[\"QObs(mm/d)\"] * 86400 / (area * 10**6)\n",
        "\n",
        "    data.set_index(['basin_id', 'Year_Mnth_Day'], inplace=True)\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "def read_discharge_data(data_dir: str):\n",
        "    files = list(Path(data_dir).glob('**/*_streamflow_qc.txt'))\n",
        "\n",
        "    data_list = []\n",
        "\n",
        "    pbar = tqdm(files, file=sys.stdout, position=0)\n",
        "    for fpath in pbar:\n",
        "        pbar.set_description(\"process \" + fpath.name)\n",
        "        basin_id = int(fpath.name.split('_')[0])\n",
        "        area = forcing_data.loc[basin_id].iloc[0]['basin_area(m2)']\n",
        "\n",
        "        data = read_discharge_file(fpath, area)\n",
        "\n",
        "        data_list.append(data)\n",
        "\n",
        "    discharge_data = pd.concat(data_list, axis=0, copy=False)\n",
        "\n",
        "    return discharge_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43ceexGqE6VR"
      },
      "source": [
        "## read attributes files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lBT_KXf6E6VU"
      },
      "outputs": [],
      "source": [
        "def read_attributes(data_dir: str):\n",
        "    files = list(Path(data_dir).glob('camels_*.txt'))\n",
        "\n",
        "    data_list = []\n",
        "\n",
        "    pbar = tqdm(files, file=sys.stdout, position=0)\n",
        "    for fpath in pbar:\n",
        "        pbar.set_description(\"process \" + fpath.name)\n",
        "        data = pd.read_csv(fpath, delimiter=\";\", index_col=\"gauge_id\")\n",
        "\n",
        "        data_list.append(data)\n",
        "\n",
        "    attributes = pd.concat(data_list, axis=1, copy=False)\n",
        "\n",
        "    return attributes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssAt85TzUOSa"
      },
      "source": [
        "# Read csv files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rDcqb9k7SekK"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x2oqng3-SekS"
      },
      "outputs": [],
      "source": [
        "!cp /content/gdrive/My\\ Drive/Caravan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aGQUvx_qSekV"
      },
      "outputs": [],
      "source": [
        "!tar xjf ./Caravan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dcWwbqvGYNCx"
      },
      "outputs": [],
      "source": [
        "BASIN_COL = 'gauge_id'\n",
        "DATE_COL = 'date'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_Nw3shzbZ8D"
      },
      "source": [
        "## Merge Individual Catchment Timeseries Files into One"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JltcU8yIbmLu"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "from tqdm import tqdm\n",
        "\n",
        "merge_catchments = False\n",
        "if merge_catchments:\n",
        "  path = \"/content/gdrive/MyDrive/Caravan/timeseries/csv/hysets\"\n",
        "  csv_files = glob.glob(path + \"/*.csv\")\n",
        "  gauge_ids = [f.split('/')[-1][:-4] for f in csv_files]\n",
        "  print(csv_files)\n",
        "  print(gauge_ids)\n",
        "\n",
        "  #combine all files in the list\n",
        "  df_original = [pd.read_csv(f) for f in tqdm(csv_files)]\n",
        "\n",
        "  df_addedGauge = [df_original[i].insert(1, 'gauge_id', gauge_ids[i]) for i in tqdm(range(len(df_original)))]\n",
        "  combined_csv = pd.concat(df for df in df_original)\n",
        "  #export to csv\n",
        "  path = \"/content/gdrive/MyDrive/Caravan/timeseries/csv/hysets_combined.csv\"\n",
        "  with open(path, 'w', encoding = 'utf-8-sig') as f:\n",
        "    combined_csv.to_csv(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAupWyNZbgRt"
      },
      "source": [
        "## Shift Date Forward by One Day"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zHd8-W85bfpF"
      },
      "outputs": [],
      "source": [
        "#camels\n",
        "#camelsbr\n",
        "#camelscl\n",
        "#hysets\n",
        "shift_date = False\n",
        "if shift_date:\n",
        "  input_df = pd.DataFrame()\n",
        "  count = 0\n",
        "  input_p = pd.DataFrame()\n",
        "  for chunk in pd.read_csv(\"/content/gdrive/MyDrive/Caravan/timeseries/csv/hysets_combined.csv\", chunksize = 100000, low_memory=False):\n",
        "    input_p = pd.concat([input_p,chunk])\n",
        "\n",
        "  input_p = input_p.set_index(pd.DatetimeIndex(input_p['date']))\n",
        "  input_p = input_p.shift(1, freq = 'D')\n",
        "  input_p['date'] = input_p.index\n",
        "  input_p = input_p.reset_index(drop = True)\n",
        "\n",
        "  path = \"/content/gdrive/MyDrive/Caravan/timeseries/csv/hysets_combined_shifted.csv\"\n",
        "  with open(path, 'w', encoding = 'utf-8-sig') as f:\n",
        "    input_p.to_csv(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAOJCuoeVq45"
      },
      "source": [
        "## Remove Dynamic Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G-6tOktkVpv5"
      },
      "outputs": [],
      "source": [
        "remove_variables = True\n",
        "nation = 'lamah'\n",
        "if remove_variables:\n",
        "  TestInputTimeSeries = np.load('/content/gdrive/My Drive/Caravan/training/{}/BasicInputTimeSeries_{}.npy'.format(nation, nation), allow_pickle = True)\n",
        "  with open('/content/gdrive/My Drive/Caravan/training/{}/metadata_{}.json'.format(nation, nation), 'r') as f:\n",
        "    metadata = json.load(f)\n",
        "\n",
        "  df = pd.DataFrame(TestInputTimeSeries, columns = metadata[\"BasicInputTimeSeries\"][\"fields\"])\n",
        "  df1 = df[['date', 'gauge_id', 'total_precipitation_sum', 'temperature_2m_mean', 'streamflow']]\n",
        "  df1['date'] = pd.to_datetime(df1['date'], format='%Y-%m-%d')\n",
        "  BasicInputTimeSeries = df1.to_numpy()\n",
        "  np.save(\"/content/gdrive/MyDrive/Caravan/training/{}/BasicInputTimeSeries_{}_unique\".format(nation, nation), BasicInputTimeSeries)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wah0jhU_w2C-"
      },
      "outputs": [],
      "source": [
        "metadata[\"BasicInputTimeSeries\"][\"fields\"] = ['date', 'gauge_id', 'total_precipitation_sum', 'temperature_2m_mean', 'streamflow']\n",
        "with open('/content/gdrive/MyDrive/Caravan/training/{}/metadata_{}_unique.json'.format(nation, nation), 'w') as outfile:\n",
        "  json.dump(metadata, outfile, indent='\\t')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljLH9Ree897k"
      },
      "source": [
        "## Choose Nation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3h4g3xly9AWM"
      },
      "outputs": [],
      "source": [
        "nation = \"camels\"   # camels, camelsaus, camelscl, camelsgb, camelsbr, hysets, lamah\n",
        "\n",
        "attr_path = \"/content/gdrive/MyDrive/Caravan/attributes/{}/attributes_caravan_{}.csv\".format(nation, nation)\n",
        "input_path = \"/content/gdrive/MyDrive/Caravan/timeseries/csv/{}_combined.csv\".format(nation)\n",
        "other_path = \"/content/gdrive/MyDrive/Caravan/attributes/{}/attributes_other_{}.csv\".format(nation, nation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KD-lkcMDVqFb"
      },
      "source": [
        "## Read Nation CSVs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c_lTZM8f366P"
      },
      "outputs": [],
      "source": [
        "# Static\n",
        "attr_p = pd.read_csv(attr_path)\n",
        "other_p = pd.read_csv(other_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sCPigvgHUVe9"
      },
      "outputs": [],
      "source": [
        "def timenow():\n",
        "  now = datetime.now()\n",
        "  return now.strftime(\"%m/%d/%Y, %H:%M:%S\") + \" UTC\"\n",
        "\n",
        "input_p = pd.DataFrame()\n",
        "iterator = 0\n",
        "if nation == \"hysets\":\n",
        "  for chunk in pd.read_csv(\"/content/gdrive/MyDrive/Caravan/timeseries/csv/hysets_combined_processed_final.csv\", chunksize = 1000000, low_memory=False):\n",
        "    fred = chunk.shape\n",
        "    print( str(iterator) + ' chunk ' + str(fred) + ' ' + timenow() )\n",
        "    iterator += 1\n",
        "    input_p = pd.concat([input_p,chunk])\n",
        "else:\n",
        "  input_p = pd.read_csv(input_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "50EWT9bKorYj"
      },
      "outputs": [],
      "source": [
        "input_p = input_p.drop(input_p.columns[0],axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7D2WzVt3yGdZ"
      },
      "outputs": [],
      "source": [
        "input_p"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VuK_rFDLMwKf"
      },
      "outputs": [],
      "source": [
        "from datetime import timedelta\n",
        "input_p.sort_values([DATE_COL, BASIN_COL], inplace=True, ignore_index=True)\n",
        "input_p['gauge_id'] = [int(x.split('_')[-1].translate({ord(i): None for i in 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'})) for x in input_p['gauge_id']]\n",
        "input_p['date'] = pd.to_datetime(input_p['date'], format='%Y-%m-%d')\n",
        "input_p = input_p[(input_p['date'] >= '1989-10-02') & (input_p['date'] <= '2008-12-31')]\n",
        "input_p.reset_index(inplace = True, drop = True)\n",
        "NaN_rows = []\n",
        "\n",
        "# remove the three catchments with NaN values in camels_us\n",
        "if nation == \"camels\":\n",
        "  input_p.drop(input_p[input_p['gauge_id'] == 'camels_03066000'].index, inplace = True)\n",
        "  input_p.drop(input_p[input_p['gauge_id'] == 'camels_03281100'].index, inplace = True)\n",
        "  input_p.drop(input_p[input_p['gauge_id'] == 'camels_12141300'].index, inplace = True)\n",
        "  input_p.reset_index(inplace = True, drop = True)\n",
        "\n",
        "  input_p = input_p.loc[input_p['gauge_id'].isin(unique)]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XNy8Dq1YK1r"
      },
      "source": [
        "## sanity check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gY4yFn_08BEs"
      },
      "outputs": [],
      "source": [
        "def find_time_range(df: pd.DataFrame):\n",
        "  groups = df[[BASIN_COL, DATE_COL]].groupby([BASIN_COL])\n",
        "  counts = groups.count()\n",
        "\n",
        "  abnormal_cols = counts[counts[DATE_COL] < 1000].index.values\n",
        "\n",
        "  if len(abnormal_cols) > 0:\n",
        "    print('abnormal cols', abnormal_cols)\n",
        "    groups = df[[BASIN_COL, DATE_COL]].groupby([BASIN_COL])\n",
        "\n",
        "  mins = groups.min()\n",
        "  maxs = groups.max()\n",
        "\n",
        "  min_d  = np.max(mins.values)\n",
        "  max_d  = np.min(maxs.values)\n",
        "  print(min_d, max_d)\n",
        "\n",
        "  return min_d, max_d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cMpNBh6NMuNE"
      },
      "outputs": [],
      "source": [
        "InitialDate, EndDate = find_time_range(input_p)\n",
        "print(\"InitialDate \", InitialDate)\n",
        "print(\"EndDate \", EndDate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "spKy5F11rwVm"
      },
      "outputs": [],
      "source": [
        "# read basin meta data\n",
        "basin_meta_p = pd.read_csv(\"/content/gdrive/MyDrive/Caravan/attributes/{}/attributes_other_{}.csv\".format(nation, nation))\n",
        "basin_meta_p.drop_duplicates(inplace=True, ignore_index=True)\n",
        "basin_meta_p['gauge_id'] = [int(x.split('_')[-1].translate({ord(i): None for i in 'ABCDEFGHJKLMNPOQRSTUVWXYZ'})) for x in basin_meta_p['gauge_id']]\n",
        "if nation == \"camels\":\n",
        "  basin_meta_p.drop(basin_meta_p[basin_meta_p['gauge_id'] == '3066000'].index, inplace = True)\n",
        "  basin_meta_p.drop(basin_meta_p[basin_meta_p['gauge_id'] == '3281100'].index, inplace = True)\n",
        "  basin_meta_p.drop(basin_meta_p[basin_meta_p['gauge_id'] == '12141300'].index, inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Zwr4ocfHxZZ"
      },
      "outputs": [],
      "source": [
        "basin_meta_p"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "miJ53_qUSzQA"
      },
      "source": [
        "## Dynamic Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4akMAxhv-pPq"
      },
      "outputs": [],
      "source": [
        "# Edit gauge ids to only contain integers\n",
        "input_p.sort_values([DATE_COL, BASIN_COL], inplace=True, ignore_index=True)\n",
        "input_p['gauge_id'] = [int(x.split('_')[-1].translate({ord(i): None for i in 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'})) for x in input_p['gauge_id']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "NUHj2WkRh-oD"
      },
      "outputs": [],
      "source": [
        "input_p"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8PaFl91P7sJP"
      },
      "outputs": [],
      "source": [
        "print(len(input_p.columns))\n",
        "print(input_p.isna().sum())\n",
        "print('Total number of data for each column: ', len(input_p['streamflow']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vz0ghk4HoSwB"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "analyze_NaN = False\n",
        "\n",
        "if analyze_NaN:\n",
        "  gauge_id = list(input_p['gauge_id'][input_p['streamflow'].isna()])\n",
        "  date = list(input_p['date'][input_p['streamflow'].isna()])\n",
        "  distinct_date = list(set(date))\n",
        "\n",
        "  count = [date.count(d) for d in distinct_date] # Very slow\n",
        "  print(count)\n",
        "  print(len(count))\n",
        "\n",
        "  plt.bar(distinct_date, count)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fonYyvpHVWU7"
      },
      "source": [
        "### NaN Sequence **Over** Catchment Graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ywiiFWYBZjUf"
      },
      "outputs": [],
      "source": [
        "# group intervals of NaN values\n",
        "def find_intervals(df, date, column):\n",
        "  m = df[column].isna()\n",
        "  r = [[*g[date]] for _, g in df[m].groupby((~m).cumsum())]\n",
        "  return r"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iB9y-G72Z5wt"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "analyze_NaN = False\n",
        "if analyze_NaN:\n",
        "  # NaN interpolation\n",
        "  us = False\n",
        "  test_df = input_p\n",
        "\n",
        "  # group dataframe by catchment\n",
        "  catchments = list(test_df['gauge_id'].unique())\n",
        "  print(len(catchments))\n",
        "  print(catchments)\n",
        "\n",
        "  NaN_intervals = []\n",
        "  ids = []\n",
        "  for c in catchments:\n",
        "    sub_df = test_df[test_df['gauge_id'] == c] # Very slow\n",
        "    ranges = find_intervals(sub_df, 'date', 'streamflow')\n",
        "    if len(ranges) > 0:\n",
        "      ranges = [(len(x)/7031)*100 for x in ranges]\n",
        "      NaN_intervals.append(ranges)\n",
        "      ids.append(c)\n",
        "\n",
        "\n",
        "  print(NaN_intervals)\n",
        "  print(len(NaN_intervals))\n",
        "  print(ids)\n",
        "\n",
        "\n",
        "  # Plot\n",
        "  ind = 0\n",
        "  width = 1\n",
        "\n",
        "  position = ind\n",
        "\n",
        "  plt.figure(figsize=(50,15))\n",
        "  for i in range(len(ids)):\n",
        "    colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k']\n",
        "    color_i = 0\n",
        "    b = 0\n",
        "\n",
        "\n",
        "    for j in range(len(NaN_intervals[i])):\n",
        "\n",
        "      cur_length = NaN_intervals[i][j]\n",
        "      if cur_length >= 0 and cur_length < 20:\n",
        "        plt.bar(position, cur_length, width, bottom = b, color = 'b')\n",
        "\n",
        "      elif cur_length >= 20 and cur_length < 50:\n",
        "        plt.bar(position, cur_length, width, bottom = b, color = 'g')\n",
        "\n",
        "      elif cur_length >= 50 and cur_length < 100:\n",
        "        plt.bar(position, cur_length, width, bottom = b, color = 'r')\n",
        "\n",
        "      else:\n",
        "        plt.bar(position, cur_length, width, bottom = b, color = 'y')\n",
        "\n",
        "      b += NaN_intervals[i][j]\n",
        "\n",
        "    position += width\n",
        "\n",
        "  plt.title('{} NaN Distribution Plot'.format(nation))\n",
        "  plt.ylabel('NaN Percentages (%)')\n",
        "  plt.xlabel('Catchment')\n",
        "  plt.legend(labels = ['Blue: x < 20%', 'Green: 20% <= x < 50%', 'Red: 50% <= x < 100% ', 'Yellow: 100%'])\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDQ0k9buIiMi"
      },
      "source": [
        "### Determine Dynamic Variable Normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0tBmHissQ4sN"
      },
      "outputs": [],
      "source": [
        "# Check RAM usage\n",
        "import sys\n",
        "\n",
        "local_vars = list(locals().items())\n",
        "for var, obj in local_vars:\n",
        "    print(var, sys.getsizeof(obj))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7YrPFWla6KxS"
      },
      "outputs": [],
      "source": [
        "input_p"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g-v0k2gVLTAA"
      },
      "outputs": [],
      "source": [
        "for chunk in pd.read_csv(\"/content/gdrive/MyDrive/Caravan/timeseries/csv/hysets_combined_shifted.csv\", chunksize = 500000, low_memory=False):\n",
        "    fred = chunk.shape\n",
        "    print( str(iterator) + ' chunk ' + str(fred) + ' ' + timenow() )\n",
        "    iterator += 1\n",
        "    input_p = pd.concat([input_p,chunk])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Ye2h9rXmeyh"
      },
      "outputs": [],
      "source": [
        "BasicInputTimeSeries = input_p.to_numpy()\n",
        "\n",
        "print(BasicInputTimeSeries.shape)\n",
        "print(BasicInputTimeSeries[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cwPhdCBoQblM"
      },
      "outputs": [],
      "source": [
        "input_p = None\n",
        "chunk = None\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xdQaASe9L5t2"
      },
      "outputs": [],
      "source": [
        "input_p.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOaq0QW5R6_9"
      },
      "source": [
        "### Check NaN-containing Sequence Fraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N2hr2R6kR6Mo"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "if analyze_NaN:\n",
        "  #TODO: Open numpy from Gdrive\n",
        "  TestInputTimeSeries = np.load('/content/gdrive/My Drive/Caravan/training/{}/BasicInputTimeSeries_{}.npy'.format(nation, nation), allow_pickle = True)\n",
        "  TestInputStaticProps = np.load('/content/gdrive/My Drive/Caravan/training/{}/BasicInputStaticProps_{}.npy'.format(nation, nation), allow_pickle = True)\n",
        "  print(len(TestInputStaticProps))\n",
        "\n",
        "  Tseq = 21\n",
        "  Nloc = len(attr_p['gauge_id'])-3\n",
        "  NuminputSeries = TestInputTimeSeries.shape[1]\n",
        "  print(NuminputSeries)\n",
        "  # InitialDate = datetime.strptime(str(InitialDate),'%Y-%m-%dT%H:%M:%S.%f000')\n",
        "  # FinalDate = datetime.strptime(str(EndDate),'%Y-%m-%dT%H:%M:%S.%f000')\n",
        "\n",
        "  init = datetime.strptime('1989-10-02T00:00:00.000000000','%Y-%m-%dT%H:%M:%S.%f000')\n",
        "  final = datetime.strptime('2008-12-31T00:00:00.000000000','%Y-%m-%dT%H:%M:%S.%f000')\n",
        "\n",
        "  NumberofTimeunits = (final-init).days + 1\n",
        "  Num_Seq = int(NumberofTimeunits - Tseq)\n",
        "\n",
        "  num_catchments = Nloc\n",
        "  RawInputSeqDimension = Tseq\n",
        "  print(num_catchments)\n",
        "\n",
        "  # TODO: Reshape\n",
        "  TestInputTimeSeries = np.delete(TestInputTimeSeries,[0,1],1)\n",
        "  TestInputTimeSeries = np.reshape(TestInputTimeSeries,[NumberofTimeunits,Nloc,NuminputSeries-2])\n",
        "\n",
        "\n",
        "  iseq = 0\n",
        "  nans = [0 for i in range(num_catchments)]\n",
        "  nan = 0\n",
        "  no_nan = 0\n",
        "\n",
        "\n",
        "  while iseq < Num_Seq:\n",
        "    icatchment = 0\n",
        "    while icatchment < num_catchments:\n",
        "      # if pd.isna(TestInputTimeSeries[iseq:iseq+Tseq, icatchment]).any():\n",
        "      #   nans[icatchment] += 1\n",
        "\n",
        "      if pd.isna(TestInputTimeSeries[iseq:iseq+Tseq, icatchment]).any():\n",
        "        nan += 1\n",
        "        nans[icatchment] += 1\n",
        "      else:\n",
        "        no_nan += 1\n",
        "\n",
        "      icatchment += 1\n",
        "\n",
        "    iseq += 1\n",
        "\n",
        "  print(nan, no_nan)\n",
        "\n",
        "\n",
        "  nan_ratios = [x/Num_Seq for x in nans]\n",
        "  plt.figure(figsize=(50,15))\n",
        "  position = 0\n",
        "  for i in range(len(nan_ratios)):\n",
        "    if nan_ratios[i] > 0.5:\n",
        "      plt.bar(position, nan_ratios[i], color = 'r')\n",
        "    else:\n",
        "      plt.bar(position, nan_ratios[i], color = 'g')\n",
        "    position += 1\n",
        "\n",
        "  plt.xlabel('catchment #')\n",
        "  plt.ylabel('nan ratio')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUfyfeqKS5E_"
      },
      "source": [
        "## Static data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ONHk-TG4sYs2"
      },
      "outputs": [],
      "source": [
        "# attr_p['gauge_id'] = [int(x.split('_')[-1]) for x in attr_p['gauge_id']]\n",
        "attr_p['gauge_id'] = [int(x.split('_')[-1].translate({ord(i): None for i in 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'})) for x in attr_p['gauge_id']]\n",
        "locs = attr_p['gauge_id']\n",
        "Nloc = len(attr_p['gauge_id'])\n",
        "\n",
        "if nation == \"camels\":\n",
        "  attr_p = attr_p.loc[attr_p['gauge_id'].isin(unique)]\n",
        "\n",
        "meta_concat = basin_meta_p.drop(columns=['gauge_id', 'gauge_name', 'gauge_lat', 'gauge_lon', 'country'])\n",
        "attr_p = pd.concat([attr_p, meta_concat], axis=1)\n",
        "\n",
        "print(len(attr_p))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-0rYRwSXt1a"
      },
      "outputs": [],
      "source": [
        "attr_p"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qgnIttqAb0H"
      },
      "source": [
        "## PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KYMxW4OLAbEo"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "\n",
        "nation_li = ['camels', 'camelsaus', 'camelsbr', 'camelscl', 'camelsgb', 'hysets', 'lamah']\n",
        "caravan_attr_combined = pd.DataFrame()\n",
        "\n",
        "# process each nation's dataset\n",
        "for nat in nation_li:\n",
        "  attr = pd.read_csv(\"/content/gdrive/MyDrive/Caravan/attributes/{}/attributes_caravan_{}.csv\".format(nat, nat))\n",
        "  other = pd.read_csv(\"/content/gdrive/MyDrive/Caravan/attributes/{}/attributes_other_{}.csv\".format(nat, nat))\n",
        "\n",
        "  if nat == \"camels\":\n",
        "    unique = [1139000, 1365000, 1664000, 2324400, 4045500,\n",
        "              4127918, 5120500, 6280300, 6431500, 6470800,\n",
        "              6479438, 6622700, 6632400, 7142300, 7197000,\n",
        "              8086290, 8190000, 8377900, 9210500, 9492400,\n",
        "              10172700, 10249300, 10259200, 10263500, 12381400,\n",
        "              13083000, 13161500, 13240000, 13313000]\n",
        "    attr['gauge_id'] = [int(x.split('_')[-1].translate({ord(i): None for i in 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'})) for x in attr['gauge_id']]\n",
        "    other['gauge_id'] = [int(x.split('_')[-1].translate({ord(i): None for i in 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'})) for x in other['gauge_id']]\n",
        "    other = other.loc[other['gauge_id'].isin(unique)]\n",
        "    attr = attr.loc[attr['gauge_id'].isin(unique)]\n",
        "\n",
        "  if nat == \"hysets\":\n",
        "    other_concat = other.drop(columns=['gauge_id', 'gauge_name', 'country'])\n",
        "    attr = pd.concat([attr, other_concat], axis=1)\n",
        "\n",
        "  else:\n",
        "    other_concat = other.drop(columns=['gauge_id', 'gauge_name', 'gauge_lat', 'gauge_lon', 'country'])\n",
        "    attr = pd.concat([attr, other_concat], axis=1)\n",
        "\n",
        "\n",
        "  if len(caravan_attr_combined) == 0:\n",
        "    caravan_attr_combined = attr\n",
        "  else:\n",
        "    caravan_attr_combined = pd.concat([caravan_attr_combined, attr], axis=0)\n",
        "\n",
        "\n",
        "# for column in caravan_attr_combined.columns:\n",
        "#   print(sum(caravan_attr_combined[column].isna()))\n",
        "\n",
        "\n",
        "# Standardize Data\n",
        "\n",
        "data = caravan_attr_combined.drop(columns = ['gauge_id'])\n",
        "data = (data - data.mean(axis = 0)) / data.std(axis = 0)\n",
        "\n",
        "\n",
        "# Calculate Covariance Matrix\n",
        "covariance_matrix = np.cov(data, ddof = 0, rowvar = False)\n",
        "\n",
        "\n",
        "# Eigendecomposition on the Covariance Matrix\n",
        "eigenvalues, eigenvectors = np.linalg.eig(covariance_matrix)\n",
        "print(eigenvalues)\n",
        "print(\"=\"*100)\n",
        "\n",
        "\n",
        "# np.argsort can only provide lowest to highest; use [::-1] to reverse the list\n",
        "order_of_importance = np.argsort(eigenvalues)[::-1]\n",
        "\n",
        "\n",
        "# utilize the sort order to sort eigenvalues and eigenvectors\n",
        "labels = data.columns\n",
        "sorted_eigenvalues = eigenvalues[order_of_importance]\n",
        "sorted_eigenvectors = eigenvectors[:,order_of_importance]\n",
        "sorted_labels = labels[order_of_importance]\n",
        "print(\"=\"*100)\n",
        "for i in range(len(sorted_labels)):\n",
        "  print(\"{} : {}\".format(sorted_labels[i], eigenvalues[i]))\n",
        "\n",
        "# import matplotlib.pyplot as plt\n",
        "# labels = data.columns\n",
        "# plt.bar(labels, eigenvalues, width = 0.4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GF4TXCzdeTK8"
      },
      "outputs": [],
      "source": [
        "BasicInputStaticProps = attr_p.to_numpy()\n",
        "NpropperTimeStatic = len(attr_p.columns) - 1\n",
        "print('NpropperTimeStatic', NpropperTimeStatic)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c54IU7kyGdWg"
      },
      "source": [
        "# save datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zcFLG_3sGhW2"
      },
      "outputs": [],
      "source": [
        "np.save(\"/content/gdrive/MyDrive/Caravan/training/{}/BasicInputTimeSeries_{}_unique\".format(nation, nation), BasicInputTimeSeries)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UAZ6EEJ1Mk4r"
      },
      "outputs": [],
      "source": [
        "BasicInputTimeSeries.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I5ZUI1AnMAbL"
      },
      "outputs": [],
      "source": [
        "type(BasicInputTimeSeries[0,3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dJJjAYlXG5K4"
      },
      "outputs": [],
      "source": [
        "np.save(\"/content/gdrive/MyDrive/Caravan/training/{}/BasicInputStaticProps_{}_unique\".format(nation, nation), BasicInputStaticProps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r6_CyUpvIvle"
      },
      "outputs": [],
      "source": [
        "str(InitialDate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "esnJp3YTMERv"
      },
      "outputs": [],
      "source": [
        "# str(BasicInputTimeSeries[1, 1] - BasicInputTimeSeries[0, 1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0rz3tWINnlW"
      },
      "outputs": [],
      "source": [
        "int(((EndDate-InitialDate + np.timedelta64(1, 'D'))/np.timedelta64(1, 'D')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i_EY4FMqC430"
      },
      "outputs": [],
      "source": [
        "str(BasicInputTimeSeries[500, 0] - BasicInputTimeSeries[0, 0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e7_gi2_8HDis"
      },
      "outputs": [],
      "source": [
        "meta_data = {\n",
        "    'Nloc': len(attr_p),\n",
        "    'locs': attr_p['gauge_id'].tolist(),\n",
        "    'loc_names': basin_meta_p['gauge_name'].tolist(),\n",
        "    'BasicInputTimeSeries':{\n",
        "      'fields': [\n",
        "\t\t\t\"date\",\n",
        "\t\t\t\"gauge_id\",\n",
        "\t\t\t\"snow_depth_water_equivalent_mean\",\n",
        "\t\t\t\"surface_net_solar_radiation_mean\",\n",
        "\t\t\t\"surface_net_thermal_radiation_mean\",\n",
        "\t\t\t\"surface_pressure_mean\",\n",
        "\t\t\t\"temperature_2m_mean\",\n",
        "\t\t\t\"dewpoint_temperature_2m_mean\",\n",
        "\t\t\t\"u_component_of_wind_10m_mean\",\n",
        "\t\t\t\"v_component_of_wind_10m_mean\",\n",
        "\t\t\t\"volumetric_soil_water_layer_1_mean\",\n",
        "\t\t\t\"volumetric_soil_water_layer_2_mean\",\n",
        "\t\t\t\"volumetric_soil_water_layer_3_mean\",\n",
        "\t\t\t\"volumetric_soil_water_layer_4_mean\",\n",
        "\t\t\t\"snow_depth_water_equivalent_min\",\n",
        "\t\t\t\"surface_net_solar_radiation_min\",\n",
        "\t\t\t\"surface_net_thermal_radiation_min\",\n",
        "\t\t\t\"surface_pressure_min\",\n",
        "\t\t\t\"temperature_2m_min\",\n",
        "\t\t\t\"dewpoint_temperature_2m_min\",\n",
        "\t\t\t\"u_component_of_wind_10m_min\",\n",
        "\t\t\t\"v_component_of_wind_10m_min\",\n",
        "\t\t\t\"volumetric_soil_water_layer_1_min\",\n",
        "\t\t\t\"volumetric_soil_water_layer_2_min\",\n",
        "\t\t\t\"volumetric_soil_water_layer_3_min\",\n",
        "\t\t\t\"volumetric_soil_water_layer_4_min\",\n",
        "\t\t\t\"snow_depth_water_equivalent_max\",\n",
        "\t\t\t\"surface_net_solar_radiation_max\",\n",
        "\t\t\t\"surface_net_thermal_radiation_max\",\n",
        "\t\t\t\"surface_pressure_max\",\n",
        "\t\t\t\"temperature_2m_max\",\n",
        "\t\t\t\"dewpoint_temperature_2m_max\",\n",
        "\t\t\t\"u_component_of_wind_10m_max\",\n",
        "\t\t\t\"v_component_of_wind_10m_max\",\n",
        "\t\t\t\"volumetric_soil_water_layer_1_max\",\n",
        "\t\t\t\"volumetric_soil_water_layer_2_max\",\n",
        "\t\t\t\"volumetric_soil_water_layer_3_max\",\n",
        "\t\t\t\"volumetric_soil_water_layer_4_max\",\n",
        "\t\t\t\"total_precipitation_sum\",\n",
        "\t\t\t\"potential_evaporation_sum\",\n",
        "\t\t\t\"streamflow\"\n",
        "\t\t  ],\n",
        "      'index_fields': [BASIN_COL, DATE_COL],\n",
        "      'initial_date': str(InitialDate),\n",
        "      'end_date': str(EndDate),\n",
        "      'time_delta': str(BasicInputTimeSeries[500, 0] - BasicInputTimeSeries[0, 0]),\n",
        "      # 'time_steps': int(((EndDate-InitialDate + np.timedelta64(1, 'D'))/np.timedelta64(1, 'D'))),\n",
        "      'time_steps': 7031,\n",
        "    },\n",
        "    'BasicInputStaticProps': {\n",
        "        'fields': attr_p.columns.values.tolist(),\n",
        "        'index_fileds': ['gauge_id'],\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YznTYdXNJaCx"
      },
      "outputs": [],
      "source": [
        "meta_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Age7D6q8KgYR"
      },
      "outputs": [],
      "source": [
        "with open('/content/gdrive/MyDrive/Caravan/training/{}/metadata_{}_unique.json'.format(nation, nation), 'w') as outfile:\n",
        "  json.dump(meta_data, outfile, indent='\\t')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PnVURYZXPXm8"
      },
      "outputs": [],
      "source": [
        "!tar cjf hysets.tar.bz2 metadata_hysets.json Basic*.npy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2X-UDVw-QA03"
      },
      "outputs": [],
      "source": [
        "!cp -f hysets.tar.bz2 /content/gdrive/My\\ Drive/Caravan/training/hysets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n7KAuk-IRN8w"
      },
      "outputs": [],
      "source": [
        "a = np.load(\"/content/gdrive/MyDrive/Caravan/training/{}/BasicInputTimeSeries_{}.npy\".format(nation, nation), allow_pickle=True)\n",
        "a.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v_Vp9Cg7f74Q"
      },
      "outputs": [],
      "source": [
        "PreparedDataFile = \"/content/gdrive/MyDrive/Caravan/timeseries/csv/{}.tar.bz2\".format(nation)\n",
        "APPLDIR = '/content/gdrive/MyDrive/Caravan/training/{}'.format(nation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XlMbHSsJgJUA"
      },
      "outputs": [],
      "source": [
        "!tar xjf $PreparedDataFile -C $APPLDIR\n",
        "!tar xjf $PreparedDataFile2 -C $APPLDIR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UbOT3es1g78b"
      },
      "outputs": [],
      "source": [
        "BasicInputStaticProps = np.load(APPLDIR + '/BasicInputStaticProps_{}.npy'.format(nation), allow_pickle = True)\n",
        "BasicInputTimeSeries = np.load(APPLDIR + '/BasicInputTimeSeries_{}.npy'.format(nation), allow_pickle = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n4n5sb6jjMuK"
      },
      "outputs": [],
      "source": [
        "BasicInputTimeSeries.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fvW7sL06EeKG"
      },
      "outputs": [],
      "source": [
        "BasicInputTimeSeries[0:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5QuRIdOyiZJa"
      },
      "outputs": [],
      "source": [
        "with open(APPLDIR + '/metadata_{}.json'.format(nation), 'r') as f:\n",
        "  metadata = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3jkX0o_MilNh"
      },
      "outputs": [],
      "source": [
        "metadata"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}